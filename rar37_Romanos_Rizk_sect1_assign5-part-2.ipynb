{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Executive Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing Steps and Features Engineering (Across Datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Removal of `offer_decline_count` on Model:**\n",
        "  - Analysis from the `train.describe()` function reveals that `offer_decline_count` has a mean and standard deviation close to zero. This suggests that this variable is unlikely to influence the model significantly, leading to its exclusion from further analysis.\n",
        "\n",
        "- **Removal of Missing Values:**\n",
        "  - Upon preprocessing, missing values were identified in four columns, each accounting for approximately 4.15% of the dataset. To address this issue, all missing values were dropped from the training dataset, preparing it for subsequent processing.\n",
        "\n",
        "- **Adding the Duration of each Session**\n",
        "  - Duration in minutes was computed for each session, representing the time from entering the website to session end, spanning multiple rows in the dataset.\n",
        "\n",
        "- **Basic Feature Extraction from `event_time` Column: Extracting the year, month, day, hour, and day name from session_start_time**\n",
        "  - Basic feature extraction was performed on the `event_time` column to derive additional insights (more time data).\n",
        "\n",
        "- **Adding a Day Range based on the New 'day' column:**\n",
        "  - An additional column named `day_range` was created to categorize sessions into thirds of the month, providing further granularity in temporal analysis.\n",
        "\n",
        "- **Conversion of Columns and Removal of Redundant Ones:**\n",
        "  - Newly added columns were converted to string format, and redundant columns such as `session_start_time`, `session_expiry_time`, and `event_time` were dropped, as they were now considered redundant.\n",
        "\n",
        "  - **Normalization of Numerical Columns:**\n",
        "  - Numerical columns were normalized to ensure uniform scales across features, preventing certain variables from disproportionately influencing the model.\n",
        "\n",
        "- **One-Hot Encoding of Categorical Columns:**\n",
        "  - Categorical columns were one-hot encoded to convert them into a numerical format suitable for machine learning algorithms, allowing the model to effectively interpret and utilize categorical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Fit and Test on the Kaggle Dataset: Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Input Layer**: \n",
        "  - 64 neurons with ReLU activation.\n",
        "  - Input shape determined by `X_train.shape[1]`.\n",
        "\n",
        "- **Hidden Layer**: \n",
        "  - 35 neurons with ReLU activation.\n",
        "\n",
        "- **Output Layer**: \n",
        "  - Single neuron with linear activation.\n",
        "\n",
        "- **Optimizer**: Adam optimizer.\n",
        "\n",
        "- **Loss Function**: Mean Squared Error (MSE).\n",
        "\n",
        "- **Epochs**: 116 epochs.\n",
        "\n",
        "- **Batch Size**: 850.\n",
        "\n",
        "- **RMSE Calculation**: \n",
        "  - Train RMSE computed using `mean_squared_error`.\n",
        "  - Lower RMSE indicates better model performance.\n",
        "\n",
        "- **Predictions**:\n",
        "- Model used to predict on `data_kaggle_test`.\n",
        "- Predictions stored in `test_predictions_df`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TRIAL Steps (Removed from the Notebook Workflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trial Steps for Preprocessing and Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Filter the dataset to keep only rows where 'page_load' is equal to 'checkout'**\n",
        "  - In an attempt to reduce the dataset size and considering the fact that we are trying to predict the total money spent by a customer (it is recorded at chekcout). I tried to remove all the rows where the 'page_type' column was different than 'Checkout', resulting in a dataset size of about 150000 rows.\n",
        "  - However, I removed this step and replaced it with a subsetting technique that will stratify based on the 'page_type' column, specifically the 'Checkout' category. This way, my subset will have the same percentage of 'Checkout' as the original Dataset\n",
        "\n",
        "**Outlier Detection in Numerical Columns:**\n",
        "  - Notably, the columns `cart_total` and `last_reward_value` exhibit the highest percentage of outliers.-\n",
        "  - However, I decided to remove this step because the number of outliers in the columns is large (around 10% for some columns). This led to overfitting as the model was not being exposed to a lot of observations (the outliers removed)\n",
        "\n",
        "- **Treatment of Extreme Values in Target Variable:**\n",
        "  - Following exploratory data analysis (EDA), extreme values in the target variable were identified, with the mean close to 140 and maximum value at 197,000.\n",
        "  - However, I decided to keep these extreme values for the same reasons I kept the outliers in the dataset.\n",
        "\n",
        "**Conversion of Pixel Count to Categorical Variable:**\n",
        "  - The pixel count of devices was deemed too precise and not sufficiently informative. Consequently, this column was converted into a categorical variable with two types: 'Mobile phones' and 'Laptops'. \n",
        "  - However, I decided to remove this step because it was modifying the number of rows in the Kaggle dataset, which is not permitted when submitting to the competition\n",
        "\n",
        "**Expansion of the 'cart_data' column:**\n",
        "- Convert the string representation of dictionaries into actual dictionaries\n",
        "- Explode the cart_data column to separate rows\n",
        "- Expand the cart_data dictionary into separate columns\n",
        "- Replace the prices with the average price per category\n",
        "- Add a 'rank' column that ranks the categories based on their count\n",
        "- However, I decided to remove this step because it was adding new rows (in case a client ordered more than one product),which is not permitted when submitting to the competition. The 'cart_data' column was later dropped as it would be considered as a categorical variable in my model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trial Steps for Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Linear Regression Model:**\n",
        "  - The RMSE on the train was relatively low\n",
        "  - However, the RMSE on test was extrmely high (in the millions).\n",
        "\n",
        "- **KNN Model:**\n",
        "  - The RMSE on the test is higher than the mean indicating a very poor model.\n",
        "  - This suggests that the KNN model is overfitting the training data.\n",
        "\n",
        "- **Random Forest Model:**\n",
        "  - The Random Forest model exhibits overfitting, as seen in the substantial gap between training and test RMSE values.\n",
        "  - Despite attempting to mitigate overfitting with Random Forest, the model still performs poorly, with an RMSE significantly higher than the target mean.\n",
        "  - Further refinement in feature selection and preprocessing steps may be necessary to address this issue.\n",
        "  - A Grid Search was done for the random forest model and it resulted in low RMSE scores for both train and test set. However, the model was taking a lot to run and it was performing poorly on the Kaggle test set\n",
        "\n",
        "- **Decision Tree Model:**\n",
        "  - The Decision Tree model shows severe overfitting, with a large discrepancy between training and test RMSE values.\n",
        "  - This indicates that the model is not generalizing well to unseen data.\n",
        "\n",
        "- **Gradient Boosting Model:**\n",
        "  - The Gradient Boosting model, although showing some improvement, still yields relatively high RMSE values on training data and testing data\n",
        "  - This suggests that the Gradient Boosting model is also overfitting the training data.\n",
        "\n",
        "- **The GradientBoostingRegressor:**\n",
        "  -The RMSE for the Gradient Boosting Models was low, however it was also overfitting\n",
        "\n",
        "**The main reason why these models were not used as final models is because of overfitting, which resulted in poor performance on the kaggle competition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Notes and Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- It is important to note the the random forest model was performing well as it is an ensemble model (good for overfitting). However, the neural network model was constantly topping the random forest on the Kaggle competition.\n",
        "\n",
        "- Dataset was too large when fitting the models. This was especially a challenge for grid searchs. This is why I decided to subset the data.\n",
        "\n",
        "- Most traditional machine learning models were overfitting, which led me to venture into neural network models\n",
        "\n",
        "- **It is important to note that the prediciton file generated from the neural network model will not be the same as the one on Kaggle. This is due to the fact that my subsetting technique (stratify on checkout) did not include any random state position, which will result in a different subset at each run. In addition, the neural network model used will yield different results after each run due to many factors such as: Random Initialization, Mini-Batch Gradient Descent, Randomness in Regularization.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing the Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JezTe6UycK1_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wESpKg-3cMXE",
        "outputId": "177c994f-988c-45fc-d6f3-42e754b3d112"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HlOS6x953wr5"
      },
      "outputs": [],
      "source": [
        "#data= pd.read_csv('/content/drive/MyDrive/data.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jo3BwtHlcfEq"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\01_Education\\\\02_AUB\\\\02_Spring2024\\\\MSBA315-Introduction to Python for Machine Learning\\\\Assignments\\\\Assignment5\\\\AssignmentFolder\\\\data\\\\data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Subset of the data by Stratifying on the 'page_type' Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming your dataset is stored in a pandas DataFrame called 'df'\n",
        "# 'stratify_column' refers to the column you want to stratify the sampling on\n",
        "\n",
        "# Calculate the proportions of each category in the stratification column\n",
        "strata_props = data['page_type'].value_counts(normalize=True)\n",
        "\n",
        "# Determine the number of samples to take from each stratum\n",
        "num_samples_per_stratum = (strata_props * 300000).round().astype(int)\n",
        "\n",
        "# Perform stratified sampling for each category\n",
        "stratified_samples = []\n",
        "for category, num_samples in num_samples_per_stratum.items():\n",
        "    stratum_samples = data[data['page_type'] == category].sample(n=num_samples, random_state=42)\n",
        "    stratified_samples.append(stratum_samples)\n",
        "\n",
        "# Concatenate the stratified samples into a single DataFrame\n",
        "data = pd.concat(stratified_samples)\n",
        "\n",
        "# Reset index if needed\n",
        "data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting Time Columns to DateTime Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xxDK3aR_fy7x"
      },
      "outputs": [],
      "source": [
        "# Convert object columns to datetime objects\n",
        "data['session_start_time'] = pd.to_datetime(data['session_start_time'])\n",
        "data['session_expiry_time'] = pd.to_datetime(data['session_expiry_time'])\n",
        "data['event_time'] = pd.to_datetime(data['event_time'])\n",
        "data['event_time_zone'] = data['event_time_zone'].astype('str')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_eQRfLmbfyLQ"
      },
      "outputs": [],
      "source": [
        "train, valid = train_test_split(data, test_size=0.2, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing and Feature Engineering for the Train Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing Zero or Near Zero Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gvyJ4053f9vj"
      },
      "outputs": [],
      "source": [
        "#mean is approx to 0 in 'offer_decline_count\"\n",
        "train = train.drop('offer_decline_count', axis=1)\n",
        "train=train.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Droping Null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eS7wCjcsgAji"
      },
      "outputs": [],
      "source": [
        "# Assuming 'data' is your DataFrame\n",
        "train.dropna(thresh=data.shape[1]-1, inplace=True)\n",
        "\n",
        "# Reset the index after dropping rows\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "train=train.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TRIAL: Outlier Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the numerical variables\n",
        "# numerical_variables = ['cart_quantity', 'cart_total', 'last_reward_value', 'last_spend_value', 'user_screen_size']\n",
        "\n",
        "# Loop over each numerical feature\n",
        "# for feature in numerical_variables:\n",
        "#     Calculate quartiles and IQR for the current feature\n",
        "#     Q1 = train[feature].quantile(0.25)\n",
        "#     Q3 = train[feature].quantile(0.75)\n",
        "#     IQR = Q3 - Q1\n",
        "\n",
        "#     Define lower and upper bounds for outliers\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "#     Filter out rows with outliers in the current feature\n",
        "#     outliers_mask = (train[feature] < lower_bound) | (train[feature] > upper_bound)\n",
        "#     train = train.drop(train[outliers_mask].index)\n",
        "\n",
        "# Reset index after dropping rows\n",
        "# train.reset_index(drop=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Define the numerical variables\n",
        "# numerical_variables = ['cart_quantity', 'cart_total', 'last_reward_value', 'last_spend_value', 'user_screen_size']\n",
        "\n",
        "# # Loop over each numerical feature\n",
        "# for feature in numerical_variables:\n",
        "#     # Calculate quartiles and IQR for the current feature\n",
        "#     Q1 = train[feature].quantile(0.25)\n",
        "#     Q3 = train[feature].quantile(0.75)\n",
        "#     IQR = Q3 - Q1\n",
        "\n",
        "#     # Define lower and upper bounds for outliers\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "#     # Replace outliers with minimum or maximum value\n",
        "#     min_val = train[feature].min()\n",
        "#     max_val = train[feature].max()\n",
        "#     train.loc[train[feature] < lower_bound, feature] = min_val\n",
        "#     train.loc[train[feature] > upper_bound, feature] = max_val\n",
        "    \n",
        "# # Reset index after modifying the DataFrame\n",
        "# train.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding the Duration of each Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pgkmKIwgLo6",
        "outputId": "3a0d2e28-5e7e-4115-d2f5-b2f5f99727ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             session_id               session_start_time  \\\n",
            "0  65394f9c-00e3-405b-8166-4df6d263927b 2020-01-29 03:55:54.348000+00:00   \n",
            "1  8213705e-4fad-46ef-aaf9-5a057399a621 2019-10-09 05:24:04.238000+00:00   \n",
            "2  12e86b40-60bb-48e5-b57c-bc50ad2e1ecc 2020-02-08 15:15:10.977000+00:00   \n",
            "3  92c9b95e-dd23-43f3-9646-40b8ed555599 2020-02-03 15:23:45.041000+00:00   \n",
            "4  a4dc3781-f2d9-4365-950e-d35b7b6a1897 2019-11-25 14:06:50.778000+00:00   \n",
            "\n",
            "               session_expiry_time  duration_minutes  \n",
            "0 2020-01-29 04:38:47.193000+00:00             77.78  \n",
            "1 2019-10-09 06:33:48.166000+00:00             69.73  \n",
            "2 2020-02-08 15:49:55.965000+00:00             34.75  \n",
            "3 2020-02-03 16:14:48.806000+00:00             63.15  \n",
            "4 2019-11-25 14:53:52.912000+00:00             66.75  \n"
          ]
        }
      ],
      "source": [
        "# Group by 'session_id' and calculate the duration\n",
        "session_durations = train.groupby('session_id').agg({\n",
        "    'session_start_time': 'min',\n",
        "    'session_expiry_time': 'max'\n",
        "}).reset_index()\n",
        "\n",
        "# Calculate the duration as the difference between the max expiry time and the min start time\n",
        "session_durations['duration'] = session_durations['session_expiry_time'] - session_durations['session_start_time']\n",
        "\n",
        "# Convert duration to a more convenient unit: minutes\n",
        "session_durations['duration_minutes'] = session_durations['duration'].dt.total_seconds() / 60  # Convert seconds to minutes\n",
        "\n",
        "# Round the duration_minutes column to 2 decimal places (nearest 0.01)\n",
        "session_durations['duration_minutes'] = session_durations['duration_minutes'].round(2)\n",
        "\n",
        "# Now merge this back into the original dataframe\n",
        "train = train.merge(session_durations[['session_id', 'duration_minutes']], on='session_id', how='left')\n",
        "\n",
        "# Verify the results\n",
        "print(train[['session_id', 'session_start_time', 'session_expiry_time', 'duration_minutes']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting the year, month, day, hour, and day name from session_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur7rQCSlgOkY",
        "outputId": "5e65e10a-cf65-46ba-a2b2-8652f13e0e30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   year  month  day  hour day_of_week\n",
            "0  2020      1   29     4   Wednesday\n",
            "1  2019     10    9     6   Wednesday\n",
            "2  2020      2    8    15    Saturday\n",
            "3  2020      2    3    15      Monday\n",
            "4  2019     11   25    14      Monday\n"
          ]
        }
      ],
      "source": [
        "# Extract year, month, day, hour, and day name from session_start_time before normalizing\n",
        "train['year'] = train['event_time'].dt.year\n",
        "train['month'] = train['event_time'].dt.month\n",
        "train['day'] = train['event_time'].dt.day\n",
        "train['hour'] = train['event_time'].dt.hour  # Extract hour before normalizing\n",
        "train['day_of_week'] = train['event_time'].dt.day_name()\n",
        "\n",
        "# Now you have separate columns for year, month, day, hour, and day_of_week along with a combined string if needed\n",
        "print(train[['year', 'month', 'day', 'hour', 'day_of_week']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding a Day Range based on the New 'day' column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfB1GUL7gQuq",
        "outputId": "043c81eb-f20a-45da-8259-980fecbab2b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   year  month  day  day_range  hour day_of_week\n",
            "0  2020      1   29          3     4   Wednesday\n",
            "1  2019     10    9          1     6   Wednesday\n",
            "2  2020      2    8          1    15    Saturday\n",
            "3  2020      2    3          1    15      Monday\n",
            "4  2019     11   25          3    14      Monday\n"
          ]
        }
      ],
      "source": [
        "# Function to categorize days with updated labels\n",
        "def categorize_day(day):\n",
        "    if 1 <= day <= 10:\n",
        "        return 1  # Updated label for days 1-10\n",
        "    elif 11 <= day <= 21:\n",
        "        return 2  # Updated label for days 11-21\n",
        "    else:\n",
        "        return 3  # Updated label for days 22-31\n",
        "\n",
        "# Apply the updated function to the 'day' column to create a new 'day_range' column with numerical labels\n",
        "train['day_range'] = train['day'].apply(categorize_day)\n",
        "\n",
        "# Display the first few rows to verify the new column\n",
        "print(train[['year', 'month', 'day', 'day_range', 'hour', 'day_of_week']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting the new date columns to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cr02uZrZgUh1"
      },
      "outputs": [],
      "source": [
        "# Assuming train is your DataFrame with 'event_time' column already converted to datetime\n",
        "train['year'] = train['event_time'].dt.year.astype(str)\n",
        "train['month'] = train['event_time'].dt.month.astype(str)\n",
        "train['day'] = train['event_time'].dt.day.astype(str)\n",
        "train['hour'] = train['event_time'].dt.hour.astype(str)\n",
        "train['day_range'] = train['day_range'].astype(str)\n",
        "\n",
        "#dropping irrelevant columns\n",
        "train.drop(columns='day', axis=1, inplace=True)\n",
        "train = train.drop(columns=['session_start_time', 'session_expiry_time', 'event_time'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing the  User Screen Size Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rezmVskkgXX9",
        "outputId": "cf26c9c5-03c4-46f2-e7fb-7d9e54211ae7"
      },
      "outputs": [],
      "source": [
        "train.drop(columns='user_screen_size', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making the 'cart_total' value for the same session consistent with the checkout one for that session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'train' is your DataFrame\n",
        "\n",
        "# Filter only the rows where 'page_type' is 'Checkout'\n",
        "checkout_rows = train[train['page_type'] == 'Checkout'][['session_id', 'cart_total']]\n",
        "\n",
        "# Merge the filtered rows back to the original DataFrame\n",
        "train = train.merge(checkout_rows, on='session_id', suffixes=('', '_checkout'), how='left')\n",
        "\n",
        "# Replace 'cart_total' with the value from 'cart_total_checkout' where available\n",
        "train['cart_total'] = train['cart_total_checkout'].fillna(train['cart_total'])\n",
        "\n",
        "# Drop the auxiliary column 'cart_total_checkout'\n",
        "train.drop(columns=['cart_total_checkout'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TRIAL: Expanding the 'cart_data' Column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import ast\n",
        "# import pandas as pd\n",
        "\n",
        "# # Convert the string representation of dictionaries into actual dictionaries\n",
        "# train['cart_data'] = train['cart_data'].apply(ast.literal_eval)\n",
        "\n",
        "# # Explode the cart_data column to separate rows\n",
        "# train = train.explode('cart_data').reset_index(drop=True)\n",
        "\n",
        "# # Expand the cart_data dictionary into separate columns\n",
        "# train = train.join(pd.json_normalize(train['cart_data']))\n",
        "\n",
        "# # Drop the original cart_data column if needed\n",
        "# train = train.drop(columns=['cart_data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Convert 'productPrice' and 'productUndiscountedPrice' columns to floats\n",
        "# train['productPrice'] = train['productPrice'].astype(np.float64)\n",
        "# train['productUndiscountedPrice'] = train['productUndiscountedPrice'].astype(np.float64)\n",
        "# train['quantity'] = train['quantity'].astype(np.int64)\n",
        "\n",
        "# # Check the data types after conversion\n",
        "# print(train[['productPrice', 'productUndiscountedPrice']].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Assuming 'train' is your DataFrame\n",
        "# # Calculate average price for each category\n",
        "# train['average_price'] = train.groupby('productCategory')['productPrice'].transform('mean')\n",
        "\n",
        "# # Replace values in 'productPrice' column with average prices for each category\n",
        "# train['productPrice'] = train['average_price']\n",
        "\n",
        "# # Drop the 'average_price' column if not needed\n",
        "# train.drop(columns=['average_price'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Assuming 'train' is your DataFrame\n",
        "# # Calculate average price for each category\n",
        "# train['average_price'] = train.groupby('productCategory')['productUndiscountedPrice'].transform('mean')\n",
        "\n",
        "# # Replace values in 'productPrice' column with average prices for each category\n",
        "# train['productUndiscountedPrice'] = train['average_price']\n",
        "\n",
        "# # Drop the 'average_price' column if not needed\n",
        "# train.drop(columns=['average_price'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Assuming 'df' is your DataFrame with the 'productCategory' column\n",
        "# # Calculate counts of each category\n",
        "# category_counts = train['productCategory'].value_counts()\n",
        "\n",
        "# # Create a DataFrame to store the category counts and their rankings\n",
        "# category_ranking = pd.DataFrame({'category': category_counts.index, 'count': category_counts.values})\n",
        "\n",
        "# # Rank the categories based on count\n",
        "# category_ranking['rank'] = category_ranking['count'].rank(ascending=False).astype(int)\n",
        "\n",
        "# # Merge the ranking DataFrame with the original DataFrame on the 'productCategory' column\n",
        "# train = pd.merge(train, category_ranking, left_on='productCategory', right_on='category', how='left')\n",
        "\n",
        "# # Drop the redundant 'category' and 'count' columns\n",
        "# train.drop(['category', 'count'], axis=1, inplace=True)\n",
        "\n",
        "# # Sort the DataFrame by the original index to restore the original order\n",
        "# train.sort_index(inplace=True)\n",
        "\n",
        "# # Display the DataFrame with the new 'rank' column\n",
        "# train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train.drop(['productCategory'], axis=1, inplace=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing more Unecessecary Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(columns=['user_id','session_id', 'cart_data'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determening the Numerical and the Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAh57H4siBhL",
        "outputId": "04ed30a8-f6a1-46be-8890-4334595f6bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['cart_quantity', 'cart_total', 'last_reward_value', 'last_spend_value',\n",
            "       'offer_display_count', 'duration_minutes'],\n",
            "      dtype='object')\n",
            "Index(['event_time_zone', 'event_type', 'page_type', 'user_status',\n",
            "       'last_offer_type', 'offer_acceptance_state', 'year', 'month', 'hour',\n",
            "       'day_of_week', 'day_range'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "numerical_cols = train.select_dtypes(include=['int64', 'float64']).columns\n",
        "# Exclude the column named \"total\"\n",
        "numerical_cols = numerical_cols[numerical_cols != 'total']\n",
        "categorical_cols = train.select_dtypes(include=['object', 'category']).columns\n",
        "print(numerical_cols)\n",
        "print(categorical_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing the Numerical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ffAPMmY0gcPl"
      },
      "outputs": [],
      "source": [
        "# Extract the numerical columns from the train DataFrame\n",
        "numerical_df = train[numerical_cols]\n",
        "\n",
        "# Convert integer variables to floats\n",
        "numerical_df = numerical_df.astype(float)\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the numerical columns\n",
        "numerical_df_normalized = pd.DataFrame(scaler.fit_transform(numerical_df), columns = numerical_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One Hot Encoding the Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iWNrtNd8gfrT"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a new DataFrame with selected categorical variables\n",
        "categorical_df = train[categorical_cols].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vaWdNVNGghta"
      },
      "outputs": [],
      "source": [
        "# Step 1: Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "categorical_df = categorical_df.astype(str)\n",
        "\n",
        "# Step 2: Fit and transform the data\n",
        "categorical_df_encoded = encoder.fit_transform(categorical_df)\n",
        "\n",
        "# Step 3: Convert the encoded data to a DataFrame\n",
        "categorical_df_encoded = pd.DataFrame(categorical_df_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_df.columns))\n",
        "\n",
        "#'categorical_df_encoded' is your new DataFrame with one-hot encoded categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joining the Preprocessed Numerical and Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fT5HLNDwgjrJ"
      },
      "outputs": [],
      "source": [
        "final_df = pd.concat([numerical_df_normalized, train['total'], categorical_df_encoded], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rGmLKPCmglxR"
      },
      "outputs": [],
      "source": [
        "final_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRJEO0y9nW4v",
        "outputId": "c395d5f6-bbb6-4361-ab3b-98a3ca389385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 239702 entries, 0 to 239701\n",
            "Data columns (total 90 columns):\n",
            " #   Column                           Non-Null Count   Dtype  \n",
            "---  ------                           --------------   -----  \n",
            " 0   cart_quantity                    239702 non-null  float64\n",
            " 1   cart_total                       239702 non-null  float64\n",
            " 2   last_reward_value                239702 non-null  float64\n",
            " 3   last_spend_value                 239702 non-null  float64\n",
            " 4   offer_display_count              239702 non-null  float64\n",
            " 5   duration_minutes                 239702 non-null  float64\n",
            " 6   total                            239702 non-null  float64\n",
            " 7   event_time_zone_-120             239702 non-null  float64\n",
            " 8   event_time_zone_-180             239702 non-null  float64\n",
            " 9   event_time_zone_-240             239702 non-null  float64\n",
            " 10  event_time_zone_-300             239702 non-null  float64\n",
            " 11  event_time_zone_-330             239702 non-null  float64\n",
            " 12  event_time_zone_-420             239702 non-null  float64\n",
            " 13  event_time_zone_-480             239702 non-null  float64\n",
            " 14  event_time_zone_-540             239702 non-null  float64\n",
            " 15  event_time_zone_-60              239702 non-null  float64\n",
            " 16  event_time_zone_-630             239702 non-null  float64\n",
            " 17  event_time_zone_-660             239702 non-null  float64\n",
            " 18  event_time_zone_-720             239702 non-null  float64\n",
            " 19  event_time_zone_-780             239702 non-null  float64\n",
            " 20  event_time_zone_-840             239702 non-null  float64\n",
            " 21  event_time_zone_0                239702 non-null  float64\n",
            " 22  event_time_zone_150              239702 non-null  float64\n",
            " 23  event_time_zone_180              239702 non-null  float64\n",
            " 24  event_time_zone_210              239702 non-null  float64\n",
            " 25  event_time_zone_240              239702 non-null  float64\n",
            " 26  event_time_zone_300              239702 non-null  float64\n",
            " 27  event_time_zone_360              239702 non-null  float64\n",
            " 28  event_time_zone_420              239702 non-null  float64\n",
            " 29  event_time_zone_480              239702 non-null  float64\n",
            " 30  event_time_zone_540              239702 non-null  float64\n",
            " 31  event_time_zone_60               239702 non-null  float64\n",
            " 32  event_time_zone_600              239702 non-null  float64\n",
            " 33  event_time_zone_660              239702 non-null  float64\n",
            " 34  event_type_CART_CHANGE           239702 non-null  float64\n",
            " 35  event_type_PAGE_LOAD             239702 non-null  float64\n",
            " 36  page_type_Cart                   239702 non-null  float64\n",
            " 37  page_type_Checkout               239702 non-null  float64\n",
            " 38  page_type_ProductDetailPage      239702 non-null  float64\n",
            " 39  page_type_ProductSubCategory     239702 non-null  float64\n",
            " 40  user_status_NC                   239702 non-null  float64\n",
            " 41  user_status_PBr                  239702 non-null  float64\n",
            " 42  user_status_PBu                  239702 non-null  float64\n",
            " 43  last_offer_type_C                239702 non-null  float64\n",
            " 44  last_offer_type_F                239702 non-null  float64\n",
            " 45  last_offer_type_P                239702 non-null  float64\n",
            " 46  last_offer_type_S                239702 non-null  float64\n",
            " 47  offer_acceptance_state_ACCEPTED  239702 non-null  float64\n",
            " 48  offer_acceptance_state_DECLINED  239702 non-null  float64\n",
            " 49  offer_acceptance_state_IGNORED   239702 non-null  float64\n",
            " 50  year_2019                        239702 non-null  float64\n",
            " 51  year_2020                        239702 non-null  float64\n",
            " 52  month_1                          239702 non-null  float64\n",
            " 53  month_10                         239702 non-null  float64\n",
            " 54  month_11                         239702 non-null  float64\n",
            " 55  month_12                         239702 non-null  float64\n",
            " 56  month_2                          239702 non-null  float64\n",
            " 57  month_3                          239702 non-null  float64\n",
            " 58  month_9                          239702 non-null  float64\n",
            " 59  hour_1                           239702 non-null  float64\n",
            " 60  hour_10                          239702 non-null  float64\n",
            " 61  hour_11                          239702 non-null  float64\n",
            " 62  hour_12                          239702 non-null  float64\n",
            " 63  hour_13                          239702 non-null  float64\n",
            " 64  hour_14                          239702 non-null  float64\n",
            " 65  hour_15                          239702 non-null  float64\n",
            " 66  hour_16                          239702 non-null  float64\n",
            " 67  hour_17                          239702 non-null  float64\n",
            " 68  hour_18                          239702 non-null  float64\n",
            " 69  hour_19                          239702 non-null  float64\n",
            " 70  hour_2                           239702 non-null  float64\n",
            " 71  hour_20                          239702 non-null  float64\n",
            " 72  hour_22                          239702 non-null  float64\n",
            " 73  hour_3                           239702 non-null  float64\n",
            " 74  hour_4                           239702 non-null  float64\n",
            " 75  hour_5                           239702 non-null  float64\n",
            " 76  hour_6                           239702 non-null  float64\n",
            " 77  hour_7                           239702 non-null  float64\n",
            " 78  hour_8                           239702 non-null  float64\n",
            " 79  hour_9                           239702 non-null  float64\n",
            " 80  day_of_week_Friday               239702 non-null  float64\n",
            " 81  day_of_week_Monday               239702 non-null  float64\n",
            " 82  day_of_week_Saturday             239702 non-null  float64\n",
            " 83  day_of_week_Sunday               239702 non-null  float64\n",
            " 84  day_of_week_Thursday             239702 non-null  float64\n",
            " 85  day_of_week_Tuesday              239702 non-null  float64\n",
            " 86  day_of_week_Wednesday            239702 non-null  float64\n",
            " 87  day_range_1                      239702 non-null  float64\n",
            " 88  day_range_2                      239702 non-null  float64\n",
            " 89  day_range_3                      239702 non-null  float64\n",
            "dtypes: float64(90)\n",
            "memory usage: 164.6 MB\n"
          ]
        }
      ],
      "source": [
        "final_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY6ZzoA_gqS1"
      },
      "source": [
        "# Preprocessing and Feature Engineering for the Spitted Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing Zero or Near Zero Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zKn6X3H2guBp"
      },
      "outputs": [],
      "source": [
        "valid = valid.reset_index(drop=True)\n",
        "valid = valid.drop('offer_decline_count', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "yzErBZJEgwbt"
      },
      "outputs": [],
      "source": [
        "valid.dropna(thresh=valid.shape[1]-1, inplace=True)\n",
        "valid.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding the Duration of each Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hFO8SNubg0Js"
      },
      "outputs": [],
      "source": [
        "# Group by 'session_id' and calculate the duration\n",
        "session_durations_valid = valid.groupby('session_id').agg({\n",
        "    'session_start_time': 'min',\n",
        "    'session_expiry_time': 'max'\n",
        "}).reset_index()\n",
        "\n",
        "# Calculate the duration as the difference between the max expiry time and the min start time\n",
        "session_durations_valid['duration'] = session_durations_valid['session_expiry_time'] - session_durations_valid['session_start_time']\n",
        "\n",
        "# Convert duration to minutes\n",
        "session_durations_valid['duration_minutes'] = session_durations_valid['duration'].dt.total_seconds() / 60\n",
        "session_durations_valid['duration_minutes'] = session_durations_valid['duration_minutes'].round(2)\n",
        "\n",
        "\n",
        "# Merge back into the original dataframe\n",
        "valid = valid.merge(session_durations_valid[['session_id', 'duration_minutes']], on='session_id', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting the year, month, day, hour, and day name from session_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_84_rcvg3cy",
        "outputId": "ff1783ac-db81-458e-96a9-14f4ed1444e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   year month  day hour day_of_week\n",
            "0  2020     3    4    6   Wednesday\n",
            "1  2020     2    3   14      Monday\n",
            "2  2019    11   20   15   Wednesday\n",
            "3  2020     1    4    5    Saturday\n",
            "4  2019    11   11    5      Monday\n"
          ]
        }
      ],
      "source": [
        "# Extract year, month, day, hour, and day name from event_time before normalizing\n",
        "valid['year'] = valid['event_time'].dt.year.astype(str)\n",
        "valid['month'] = valid['event_time'].dt.month.astype(str)\n",
        "valid['day'] = valid['event_time'].dt.day\n",
        "valid['hour'] = valid['event_time'].dt.hour.astype(str)\n",
        "valid['day_of_week'] = valid['event_time'].dt.day_name()\n",
        "\n",
        "# Now you have separate columns for year, month, day, hour, and day_of_week along with a combined string if needed\n",
        "print(valid[['year', 'month', 'day', 'hour', 'day_of_week']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding a Day Range based on the New 'day' column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "p74JKAS_g5ym"
      },
      "outputs": [],
      "source": [
        "# Function to categorize days with updated labels\n",
        "def categorize_day(day):\n",
        "    if 1 <= day <= 10:\n",
        "        return 1  # Updated label for days 1-10\n",
        "    elif 11 <= day <= 21:\n",
        "        return 2  # Updated label for days 11-21\n",
        "    else:\n",
        "        return 3  # Updated label for days 22-31\n",
        "\n",
        "# Apply the updated function to the 'day' column to create a new 'day_range' column with numerical labels\n",
        "valid['day_range'] = valid['day'].apply(categorize_day)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting the new date columns to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming valid is your DataFrame with 'event_time' column already converted to datetime\n",
        "valid['year'] = valid['event_time'].dt.year.astype(str)\n",
        "valid['month'] = valid['event_time'].dt.month.astype(str)\n",
        "valid['day'] = valid['event_time'].dt.day.astype(str)\n",
        "valid['hour'] = valid['event_time'].dt.hour.astype(str)\n",
        "valid['day_range'] = valid['day_range'].astype(str)\n",
        "\n",
        "valid.drop(columns=['day'], axis = 1, inplace=True)\n",
        "\n",
        "valid = valid.drop(columns=['session_start_time', 'session_expiry_time', 'event_time'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing the  User Screen Size Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LRDqpKL_g8gJ"
      },
      "outputs": [],
      "source": [
        "valid.drop(columns='user_screen_size', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making the 'cart_total' value for the same session consistent with the checkout one for that session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'train' is your DataFrame\n",
        "\n",
        "# Filter only the rows where 'page_type' is 'Checkout'\n",
        "checkout_rows = valid[valid['page_type'] == 'Checkout'][['session_id', 'cart_total']]\n",
        "\n",
        "# Merge the filtered rows back to the original DataFrame\n",
        "valid = valid.merge(checkout_rows, on='session_id', suffixes=('', '_checkout'), how='left')\n",
        "\n",
        "# Replace 'cart_total' with the value from 'cart_total_checkout' where available\n",
        "valid['cart_total'] = valid['cart_total_checkout'].fillna(valid['cart_total'])\n",
        "\n",
        "# Drop the auxiliary column 'cart_total_checkout'\n",
        "valid.drop(columns=['cart_total_checkout'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TRIAL: Expanding the 'cart_data' Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import ast\n",
        "# import pandas as pd\n",
        "\n",
        "# # Convert the string representation of dictionaries into actual dictionaries\n",
        "# valid['cart_data'] = valid['cart_data'].apply(ast.literal_eval)\n",
        "\n",
        "# # Explode the cart_data column to separate rows\n",
        "# valid = valid.explode('cart_data').reset_index(drop=True)\n",
        "\n",
        "# # Expand the cart_data dictionary into separate columns\n",
        "# valid = valid.join(pd.json_normalize(valid['cart_data']))\n",
        "\n",
        "# # Drop the original cart_data column if needed\n",
        "# valid = valid.drop(columns=['cart_data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Convert 'productPrice' and 'productUndiscountedPrice' columns to floats\n",
        "# valid['productPrice'] = valid['productPrice'].astype(np.float64)\n",
        "# valid['productUndiscountedPrice'] = valid['productUndiscountedPrice'].astype(np.float64)\n",
        "# valid['quantity'] = valid['quantity'].astype(np.int64)\n",
        "\n",
        "# # Check the data types after conversion\n",
        "# print(valid[['productPrice', 'productUndiscountedPrice']].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Assuming 'valid' is your DataFrame\n",
        "# # Calculate average price for each category\n",
        "# valid['average_price'] = valid.groupby('productCategory')['productPrice'].transform('mean')\n",
        "\n",
        "# # Replace values in 'productPrice' column with average prices for each category\n",
        "# valid['productPrice'] = valid['average_price']\n",
        "\n",
        "# # Drop the 'average_price' column if not needed\n",
        "# valid.drop(columns=['average_price'], inplace=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Assuming 'valid' is your DataFrame\n",
        "# # Calculate average price for each category\n",
        "# valid['average_price'] = valid.groupby('productCategory')['productUndiscountedPrice'].transform('mean')\n",
        "\n",
        "# # Replace values in 'productPrice' column with average prices for each category\n",
        "# valid['productUndiscountedPrice'] = valid['average_price']\n",
        "\n",
        "# # Drop the 'average_price' column if not needed\n",
        "# valid.drop(columns=['average_price'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Assuming 'valid' is your DataFrame with the 'productCategory' column\n",
        "# # Calculate counts of each category\n",
        "# category_counts = valid['productCategory'].value_counts()\n",
        "\n",
        "# # Create a DataFrame to store the category counts and their rankings\n",
        "# category_ranking = pd.DataFrame({'category': category_counts.index, 'count': category_counts.values})\n",
        "\n",
        "# # Rank the categories based on count\n",
        "# category_ranking['rank'] = category_ranking['count'].rank(ascending=False).astype(int)\n",
        "\n",
        "# # Merge the ranking DataFrame with the original DataFrame on the 'productCategory' column\n",
        "# valid = pd.merge(valid, category_ranking, left_on='productCategory', right_on='category', how='left')\n",
        "\n",
        "# # Drop the redundant 'category' and 'count' columns\n",
        "# valid.drop(['category', 'count'], axis=1, inplace=True)\n",
        "\n",
        "# # Sort the DataFrame by the original index to restore the original order\n",
        "# valid.sort_index(inplace=True)\n",
        "\n",
        "# # Display the DataFrame with the new 'rank' column\n",
        "# valid.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "#valid.drop(['productCategory'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing more Unecessecary Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid.drop(columns=['user_id','session_id', 'cart_data'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determening the Numerical and the Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzBonyLlhAtw",
        "outputId": "08be6b2e-2023-4831-ce08-7168da0d82ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['cart_quantity', 'cart_total', 'last_reward_value', 'last_spend_value',\n",
            "       'offer_display_count', 'duration_minutes'],\n",
            "      dtype='object')\n",
            "Index(['event_time_zone', 'event_type', 'page_type', 'user_status',\n",
            "       'last_offer_type', 'offer_acceptance_state', 'year', 'month', 'hour',\n",
            "       'day_of_week', 'day_range'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "numerical_cols_valid = valid.select_dtypes(include=['int64', 'float64']).columns\n",
        "# Exclude the column named \"total\"\n",
        "numerical_cols_valid = numerical_cols_valid[numerical_cols_valid != 'total']\n",
        "categorical_cols_valid = valid.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "print(numerical_cols_valid)\n",
        "print(categorical_cols_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing the Numerical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "WxSK8UqthBRl"
      },
      "outputs": [],
      "source": [
        "# Extract the numerical columns from the valid DataFrame\n",
        "numerical_df_valid = valid[numerical_cols_valid].copy()\n",
        "\n",
        "# Convert integer variables to floats if necessary\n",
        "numerical_df_valid = numerical_df_valid.astype(float)\n",
        "\n",
        "# Transform the numerical columns using the fitted scaler\n",
        "numerical_df_normalized_valid = pd.DataFrame(scaler.transform(numerical_df_valid), columns=numerical_cols_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One Hot Encoding the Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Yx5j4EKJhEsv"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a new DataFrame with selected categorical variables\n",
        "categorical_df_valid = valid[categorical_cols].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "HB8uNklWhGys"
      },
      "outputs": [],
      "source": [
        "# Assuming 'categorical_df' is your DataFrame with only categorical features\n",
        "\n",
        "# Step 1: Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Step 2: Fit and transform the data\n",
        "categorical_df_encoded_valid = encoder.fit_transform(categorical_df_valid)\n",
        "\n",
        "# Step 3: Convert the encoded data to a DataFrame\n",
        "categorical_df_encoded_valid = pd.DataFrame(categorical_df_encoded_valid.toarray(), columns=encoder.get_feature_names_out(categorical_df_valid.columns))\n",
        "\n",
        "# 'categorical_df_encoded' is your new DataFrame with one-hot encoded categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joining the Preprocessed Numerical and Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "A84Z1RcXhJrs"
      },
      "outputs": [],
      "source": [
        "final_df_valid = pd.concat([numerical_df_normalized_valid,valid['total'], categorical_df_encoded_valid], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "97DW7wVvhMF7"
      },
      "outputs": [],
      "source": [
        "final_df_valid.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cart_quantity</th>\n",
              "      <th>cart_total</th>\n",
              "      <th>last_reward_value</th>\n",
              "      <th>last_spend_value</th>\n",
              "      <th>offer_display_count</th>\n",
              "      <th>duration_minutes</th>\n",
              "      <th>total</th>\n",
              "      <th>event_time_zone_-120</th>\n",
              "      <th>event_time_zone_-180</th>\n",
              "      <th>event_time_zone_-240</th>\n",
              "      <th>...</th>\n",
              "      <th>day_of_week_Friday</th>\n",
              "      <th>day_of_week_Monday</th>\n",
              "      <th>day_of_week_Saturday</th>\n",
              "      <th>day_of_week_Sunday</th>\n",
              "      <th>day_of_week_Thursday</th>\n",
              "      <th>day_of_week_Tuesday</th>\n",
              "      <th>day_of_week_Wednesday</th>\n",
              "      <th>day_range_1</th>\n",
              "      <th>day_range_2</th>\n",
              "      <th>day_range_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.012821</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071951</td>\n",
              "      <td>96.67</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.001805</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059829</td>\n",
              "      <td>49.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.001910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025641</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007153</td>\n",
              "      <td>104.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.003282</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025641</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.049920</td>\n",
              "      <td>99.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.001075</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055341</td>\n",
              "      <td>59.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  90 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   cart_quantity  cart_total  last_reward_value  last_spend_value  \\\n",
              "0       0.008929    0.001178                0.0          0.012821   \n",
              "1       0.008929    0.001805                0.0          0.000000   \n",
              "2       0.008929    0.001910                0.0          0.025641   \n",
              "3       0.035714    0.003282                0.0          0.025641   \n",
              "4       0.008929    0.001075                0.0          0.000000   \n",
              "\n",
              "   offer_display_count  duration_minutes   total  event_time_zone_-120  \\\n",
              "0             0.000000          0.071951   96.67                   0.0   \n",
              "1             0.000000          0.059829   49.33                   0.0   \n",
              "2             0.000000          0.007153  104.98                   0.0   \n",
              "3             0.015873          0.049920   99.98                   0.0   \n",
              "4             0.000000          0.055341   59.98                   0.0   \n",
              "\n",
              "   event_time_zone_-180  event_time_zone_-240  ...  day_of_week_Friday  \\\n",
              "0                   0.0                   0.0  ...                 0.0   \n",
              "1                   0.0                   0.0  ...                 0.0   \n",
              "2                   0.0                   0.0  ...                 0.0   \n",
              "3                   0.0                   0.0  ...                 0.0   \n",
              "4                   0.0                   0.0  ...                 0.0   \n",
              "\n",
              "   day_of_week_Monday  day_of_week_Saturday  day_of_week_Sunday  \\\n",
              "0                 0.0                   0.0                 0.0   \n",
              "1                 0.0                   0.0                 0.0   \n",
              "2                 0.0                   1.0                 0.0   \n",
              "3                 1.0                   0.0                 0.0   \n",
              "4                 1.0                   0.0                 0.0   \n",
              "\n",
              "   day_of_week_Thursday  day_of_week_Tuesday  day_of_week_Wednesday  \\\n",
              "0                   0.0                  0.0                    1.0   \n",
              "1                   0.0                  0.0                    1.0   \n",
              "2                   0.0                  0.0                    0.0   \n",
              "3                   0.0                  0.0                    0.0   \n",
              "4                   0.0                  0.0                    0.0   \n",
              "\n",
              "   day_range_1  day_range_2  day_range_3  \n",
              "0          0.0          0.0          1.0  \n",
              "1          1.0          0.0          0.0  \n",
              "2          1.0          0.0          0.0  \n",
              "3          1.0          0.0          0.0  \n",
              "4          0.0          0.0          1.0  \n",
              "\n",
              "[5 rows x 90 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting the Data Set into X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "pJwXaGX_hP7u"
      },
      "outputs": [],
      "source": [
        "X_train = final_df.drop(columns=['total'])  # Selecting all columns except 'total'\n",
        "y_train = final_df['total']\n",
        "X_test=final_df_valid.drop(columns=['total'])\n",
        "y_test= final_df_valid['total']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fixing Potential Columns Discrepencie between X_train and X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZMDEhJri0q7",
        "outputId": "ac3bf32c-a053-41b9-8ec2-1fdfded35b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns present in X_train but not in X_test: {'event_time_zone_-840', 'event_time_zone_-420', 'hour_22', 'event_time_zone_-780', 'event_time_zone_-240', 'event_time_zone_-660'}\n",
            "Columns present in X_test but not in X_train: {'event_time_zone_-210'}\n"
          ]
        }
      ],
      "source": [
        "# Get the columns of X_train and X_test\n",
        "train_columns = set(X_train.columns)\n",
        "test_columns = set(X_test.columns)\n",
        "\n",
        "# Columns present in X_train but not in X_test\n",
        "missing_in_test = train_columns - test_columns\n",
        "\n",
        "# Columns present in X_test but not in X_train\n",
        "missing_in_train = test_columns - train_columns\n",
        "\n",
        "print(\"Columns present in X_train but not in X_test:\", missing_in_test)\n",
        "print(\"Columns present in X_test but not in X_train:\", missing_in_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "d-6V2WgRjEZb"
      },
      "outputs": [],
      "source": [
        "# Assuming X_train is your DataFrame and missing_in_test is your list of column names to drop\n",
        "X_train.drop(columns=missing_in_test, inplace=True)\n",
        "X_test.drop(columns=missing_in_train, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw5xMXGI_VIB",
        "outputId": "9a699d3f-6c98-40f5-8e9a-70e8f1049cd8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7491/7491\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 888us/step\n",
            "\u001b[1m1809/1809\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718us/step\n",
            "Training RMSE: 439.1727834785868\n",
            "Testing RMSE: 195.13961976756286\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network architecture\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(35, activation='relu'),\n",
        "    Dense(1)  # Output layer with one neuron for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=116, batch_size=850, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "# Predict on training and testing data\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Compute RMSE\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
        "\n",
        "print(\"Training RMSE:\", train_rmse)\n",
        "print(\"Testing RMSE:\", test_rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "700J-zlciX3S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "218.6267556315644"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TRIAL: Different Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# # Train your model (Linear Regression)\n",
        "# model = LinearRegression()\n",
        "# model.fit(X_train_pca, y_train)\n",
        "\n",
        "# # Predictions\n",
        "# train_predictions = model.predict(X_train_pca)\n",
        "# test_predictions = model.predict(X_test_pca)\n",
        "\n",
        "# # Compute Mean Squared Error (MSE)\n",
        "# train_mse = mean_squared_error(y_train, train_predictions)\n",
        "# test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "# # Compute Root Mean Squared Error (RMSE)\n",
        "# train_rmse = np.sqrt(train_mse)\n",
        "# test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# print(\"Training RMSE (Linear Regression):\", train_rmse)\n",
        "# print(\"Testing RMSE (Linear Regression):\", test_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# # Adjusting the Decision Tree parameters\n",
        "# model = DecisionTreeRegressor(\n",
        "#     random_state=42,\n",
        "#     max_depth=8,  # Example: limit the depth of the tree\n",
        "#     min_samples_split=15,  # Example: require at least 10 samples to split a node\n",
        "#     min_samples_leaf=8  # Example: require at least 5 samples at each leaf\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(X_train_pca, y_train)\n",
        "\n",
        "# # Predictions\n",
        "# train_predictions = model.predict(X_train_pca)\n",
        "# test_predictions = model.predict(X_test_pca)\n",
        "\n",
        "# # Compute Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
        "# train_mse = mean_squared_error(y_train, train_predictions)\n",
        "# test_mse = mean_squared_error(y_test, test_predictions)\n",
        "# train_rmse = np.sqrt(train_mse)\n",
        "# test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# print(\"Training RMSE:\", train_rmse)\n",
        "# print(\"Testing RMSE:\", test_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# import numpy as np\n",
        "\n",
        "# # Adjusting the Decision Tree parameters\n",
        "# model = DecisionTreeRegressor(\n",
        "#     random_state=42,\n",
        "#     max_depth=8,  # Limit the depth of the tree\n",
        "#     min_samples_split=13,  # Require at least 20 samples to split a node\n",
        "#     min_samples_leaf=5  # Require at least 5 samples at each leaf\n",
        "# )\n",
        "\n",
        "# # Train the model on the original data\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# # Predictions on the original data\n",
        "# train_predictions = model.predict(X_train)\n",
        "# test_predictions = model.predict(X_test)\n",
        "\n",
        "# # Compute Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
        "# train_mse = mean_squared_error(y_train, train_predictions)\n",
        "# test_mse = mean_squared_error(y_test, test_predictions)\n",
        "# train_rmse = np.sqrt(train_mse)\n",
        "# test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "# print(\"Training RMSE:\", train_rmse)\n",
        "# print(\"Testing RMSE:\", test_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# import numpy as np\n",
        "\n",
        "# # Setting up the Random Forest Regressor\n",
        "# rf_model = RandomForestRegressor(\n",
        "#     random_state=42,\n",
        "#     n_estimators=90,  # Number of trees in the forest\n",
        "#     max_depth=8,  # Limit the depth of each tree to prevent overfitting\n",
        "#     min_samples_split=15,  # Minimum number of samples required to split an internal node\n",
        "#     min_samples_leaf=5  # Minimum number of samples required to be at a leaf node\n",
        "# )\n",
        "\n",
        "# # Training the model on the original training data\n",
        "# rf_model.fit(X_train, y_train)\n",
        "\n",
        "# # Making predictions on both training and testing data\n",
        "# rf_train_predictions = rf_model.predict(X_train)\n",
        "# rf_test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# # Calculating Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
        "# rf_train_mse = mean_squared_error(y_train, rf_train_predictions)\n",
        "# rf_test_mse = mean_squared_error(y_test, rf_test_predictions)\n",
        "# rf_train_rmse = np.sqrt(rf_train_mse)\n",
        "# rf_test_rmse = np.sqrt(rf_test_mse)\n",
        "\n",
        "# # Outputting the RMSE results\n",
        "# print(\"Random Forest Training RMSE:\", rf_train_rmse)\n",
        "# print(\"Random Forest Testing RMSE:\", rf_test_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #Define the KNN model\n",
        "# knn = KNeighborsRegressor()\n",
        "\n",
        "# #Define the grid of hyperparameters to search\n",
        "# param_grid = {\n",
        "#    'n_neighbors': [3, 5, 7, 9],  # Number of neighbors to consider\n",
        "#    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
        "#    'p': [1, 2]  # Power parameter for the Minkowski metric\n",
        "# }\n",
        "\n",
        "# #Perform grid search cross-validation\n",
        "# grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# #Get the best hyperparameters\n",
        "# best_params = grid_search.best_params_\n",
        "# print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# #Use the best model to make predictions\n",
        "# best_model = grid_search.best_estimator_\n",
        "# y_pred_train = best_model.predict(X_train)\n",
        "# y_pred_test = best_model.predict(X_test)\n",
        "\n",
        "# #Calculate RMSE on training and test data\n",
        "# rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "# rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "# print(\"RMSE on training data:\", rmse_train)\n",
        "# print(\"RMSE on test data:\", rmse_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# # Define the KNN model with a specific number of neighbors\n",
        "# knn = KNeighborsRegressor(n_neighbors=30)  # You can change the value of n_neighbors as per your choice\n",
        "\n",
        "# # Train the model\n",
        "# knn.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred_train = knn.predict(X_train)\n",
        "# y_pred_test = knn.predict(X_test)\n",
        "\n",
        "# # Calculate RMSE on training and test data\n",
        "# rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "# rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "# print(\"RMSE on training data:\", rmse_train)\n",
        "# print(\"RMSE on test data:\", rmse_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing and Feature Engineering for the Spitted Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading the Kaggle Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_kaggle = pd.read_csv(\"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\01_Education\\\\02_AUB\\\\02_Spring2024\\\\MSBA315-Introduction to Python for Machine Learning\\\\Assignments\\\\Assignment5\\\\AssignmentFolder\\\\data\\\\test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading the Train Set to Retrain the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train_kaggle = pd.read_csv('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\01_Education\\\\02_AUB\\\\02_Spring2024\\\\MSBA315-Introduction to Python for Machine Learning\\\\Assignments\\\\Assignment5\\\\AssignmentFolder\\\\data\\\\data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>col1</th>\n",
              "      <th>col2</th>\n",
              "      <th>col3</th>\n",
              "      <th>col4</th>\n",
              "      <th>col5</th>\n",
              "      <th>col6</th>\n",
              "      <th>col7</th>\n",
              "      <th>col8</th>\n",
              "      <th>col9</th>\n",
              "      <th>col10</th>\n",
              "      <th>col11</th>\n",
              "      <th>col12</th>\n",
              "      <th>col13</th>\n",
              "      <th>col14</th>\n",
              "      <th>col15</th>\n",
              "      <th>col16</th>\n",
              "      <th>col17</th>\n",
              "      <th>col18</th>\n",
              "      <th>col19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>785cc174-f549-4528-9ddf-6cbbd379212e</td>\n",
              "      <td>6777e217-dca4-4443-b60f-91ca4cf08136</td>\n",
              "      <td>2019-08-04T03:24:37.048Z</td>\n",
              "      <td>2019-08-04T04:29:38.875Z</td>\n",
              "      <td>2019-08-04T03:59:59.963Z</td>\n",
              "      <td>180</td>\n",
              "      <td>PAGE_LOAD</td>\n",
              "      <td>ProductDetailPage</td>\n",
              "      <td>0</td>\n",
              "      <td>PBu</td>\n",
              "      <td>7</td>\n",
              "      <td>307.61</td>\n",
              "      <td>[{'productPrice': '44.99', 'productCategory': ...</td>\n",
              "      <td>C</td>\n",
              "      <td>5.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.145728e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>b0d0ee4e-9472-467e-91bc-bdf77f8a84c0</td>\n",
              "      <td>a4260259-961a-4ab1-9dd6-50a9f0c05079</td>\n",
              "      <td>2019-08-04T03:32:40.740Z</td>\n",
              "      <td>2019-08-04T04:29:42.477Z</td>\n",
              "      <td>2019-08-04T04:00:05.579Z</td>\n",
              "      <td>240</td>\n",
              "      <td>CART_CHANGE</td>\n",
              "      <td>Cart</td>\n",
              "      <td>0</td>\n",
              "      <td>PBu</td>\n",
              "      <td>5</td>\n",
              "      <td>349.62</td>\n",
              "      <td>[{'productPrice': '39.99', 'productCategory': ...</td>\n",
              "      <td>P</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.049088e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>8fa04b12-e187-405c-a30e-b27f677ed12f</td>\n",
              "      <td>4f723b51-a83d-4863-860d-f3f3419fcade</td>\n",
              "      <td>2019-08-04T03:55:34.356Z</td>\n",
              "      <td>2019-08-04T04:29:15.550Z</td>\n",
              "      <td>2019-08-04T04:00:06.161Z</td>\n",
              "      <td>420</td>\n",
              "      <td>PAGE_LOAD</td>\n",
              "      <td>Checkout</td>\n",
              "      <td>0</td>\n",
              "      <td>PBr</td>\n",
              "      <td>1</td>\n",
              "      <td>219.99</td>\n",
              "      <td>[{'productPrice': '219.99', 'productCategory':...</td>\n",
              "      <td>C</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.145728e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>304824f0-0556-4fda-a616-a27a1ad31dad</td>\n",
              "      <td>6b6e9f89-53c9-4005-9b60-a6e23c01c90f</td>\n",
              "      <td>2019-08-04T03:16:58.809Z</td>\n",
              "      <td>2019-08-04T04:29:36.673Z</td>\n",
              "      <td>2019-08-04T04:00:13.654Z</td>\n",
              "      <td>420</td>\n",
              "      <td>PAGE_LOAD</td>\n",
              "      <td>ProductSubCategory</td>\n",
              "      <td>0</td>\n",
              "      <td>PBr</td>\n",
              "      <td>1</td>\n",
              "      <td>39.99</td>\n",
              "      <td>[{'productPrice': '39.99', 'productCategory': ...</td>\n",
              "      <td>S</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.592000e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3c09349d-a803-4663-8dc3-2eb93732bb4e</td>\n",
              "      <td>92909fbd-b918-4b9b-aba8-a1d67493ba6f</td>\n",
              "      <td>2019-08-04T03:03:04.102Z</td>\n",
              "      <td>2019-08-04T04:29:04.603Z</td>\n",
              "      <td>2019-08-04T04:00:16.768Z</td>\n",
              "      <td>240</td>\n",
              "      <td>PAGE_LOAD</td>\n",
              "      <td>ProductSubCategory</td>\n",
              "      <td>0</td>\n",
              "      <td>PBu</td>\n",
              "      <td>2</td>\n",
              "      <td>79.32</td>\n",
              "      <td>[{'productPrice': '39.33', 'productCategory': ...</td>\n",
              "      <td>C</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.024634e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                  col1  \\\n",
              "0   0  785cc174-f549-4528-9ddf-6cbbd379212e   \n",
              "1   1  b0d0ee4e-9472-467e-91bc-bdf77f8a84c0   \n",
              "2   2  8fa04b12-e187-405c-a30e-b27f677ed12f   \n",
              "3   3  304824f0-0556-4fda-a616-a27a1ad31dad   \n",
              "4   4  3c09349d-a803-4663-8dc3-2eb93732bb4e   \n",
              "\n",
              "                                   col2                      col3  \\\n",
              "0  6777e217-dca4-4443-b60f-91ca4cf08136  2019-08-04T03:24:37.048Z   \n",
              "1  a4260259-961a-4ab1-9dd6-50a9f0c05079  2019-08-04T03:32:40.740Z   \n",
              "2  4f723b51-a83d-4863-860d-f3f3419fcade  2019-08-04T03:55:34.356Z   \n",
              "3  6b6e9f89-53c9-4005-9b60-a6e23c01c90f  2019-08-04T03:16:58.809Z   \n",
              "4  92909fbd-b918-4b9b-aba8-a1d67493ba6f  2019-08-04T03:03:04.102Z   \n",
              "\n",
              "                       col4                      col5  col6         col7  \\\n",
              "0  2019-08-04T04:29:38.875Z  2019-08-04T03:59:59.963Z   180    PAGE_LOAD   \n",
              "1  2019-08-04T04:29:42.477Z  2019-08-04T04:00:05.579Z   240  CART_CHANGE   \n",
              "2  2019-08-04T04:29:15.550Z  2019-08-04T04:00:06.161Z   420    PAGE_LOAD   \n",
              "3  2019-08-04T04:29:36.673Z  2019-08-04T04:00:13.654Z   420    PAGE_LOAD   \n",
              "4  2019-08-04T04:29:04.603Z  2019-08-04T04:00:16.768Z   240    PAGE_LOAD   \n",
              "\n",
              "                 col8  col9 col10  col11   col12  \\\n",
              "0   ProductDetailPage     0   PBu      7  307.61   \n",
              "1                Cart     0   PBu      5  349.62   \n",
              "2            Checkout     0   PBr      1  219.99   \n",
              "3  ProductSubCategory     0   PBr      1   39.99   \n",
              "4  ProductSubCategory     0   PBu      2   79.32   \n",
              "\n",
              "                                               col13 col14  col15  col16  \\\n",
              "0  [{'productPrice': '44.99', 'productCategory': ...     C    5.0  125.0   \n",
              "1  [{'productPrice': '39.99', 'productCategory': ...     P    5.0   50.0   \n",
              "2  [{'productPrice': '219.99', 'productCategory':...     C   10.0  100.0   \n",
              "3  [{'productPrice': '39.99', 'productCategory': ...     S    5.0   50.0   \n",
              "4  [{'productPrice': '39.33', 'productCategory': ...     C    5.0   50.0   \n",
              "\n",
              "   col17         col18    col19  \n",
              "0    1.0  3.145728e+06  IGNORED  \n",
              "1    1.0  1.049088e+06  IGNORED  \n",
              "2    1.0  3.145728e+06  IGNORED  \n",
              "3    1.0  2.592000e+06  IGNORED  \n",
              "4    1.0  1.024634e+06  IGNORED  "
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_kaggle.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetching the Column Names of the Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['user_id',\n",
              " 'session_id',\n",
              " 'session_start_time',\n",
              " 'session_expiry_time',\n",
              " 'event_time',\n",
              " 'event_time_zone',\n",
              " 'event_type',\n",
              " 'page_type',\n",
              " 'offer_decline_count',\n",
              " 'user_status',\n",
              " 'cart_quantity',\n",
              " 'cart_total',\n",
              " 'cart_data',\n",
              " 'last_offer_type',\n",
              " 'last_reward_value',\n",
              " 'last_spend_value',\n",
              " 'offer_display_count',\n",
              " 'user_screen_size',\n",
              " 'offer_acceptance_state']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "column_names = data_train_kaggle.columns.tolist()\n",
        "column_names = column_names[:-1]\n",
        "column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seperating the ID and the rest of the Features in the Kaggle Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a new DataFrame with only the first column\n",
        "first_column_df = data_kaggle.iloc[:, :1]\n",
        "\n",
        "# Create a new DataFrame with all columns except the first column\n",
        "new_df =data_kaggle.iloc[:, 1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replacing the Column Names on the Kaggle Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename columns of the DataFrame\n",
        "new_df.columns = column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>session_id</th>\n",
              "      <th>session_start_time</th>\n",
              "      <th>session_expiry_time</th>\n",
              "      <th>event_time</th>\n",
              "      <th>event_time_zone</th>\n",
              "      <th>event_type</th>\n",
              "      <th>page_type</th>\n",
              "      <th>offer_decline_count</th>\n",
              "      <th>user_status</th>\n",
              "      <th>cart_quantity</th>\n",
              "      <th>cart_total</th>\n",
              "      <th>cart_data</th>\n",
              "      <th>last_offer_type</th>\n",
              "      <th>last_reward_value</th>\n",
              "      <th>last_spend_value</th>\n",
              "      <th>offer_display_count</th>\n",
              "      <th>user_screen_size</th>\n",
              "      <th>offer_acceptance_state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>785cc174-f549-4528-9ddf-6cbbd379212e</td>\n",
              "      <td>6777e217-dca4-4443-b60f-91ca4cf08136</td>\n",
              "      <td>2019-08-04T03:24:37.048Z</td>\n",
              "      <td>2019-08-04T04:29:38.875Z</td>\n",
              "      <td>2019-08-04T03:59:59.963Z</td>\n",
              "      <td>180</td>\n",
              "      <td>PAGE_LOAD</td>\n",
              "      <td>ProductDetailPage</td>\n",
              "      <td>0</td>\n",
              "      <td>PBu</td>\n",
              "      <td>7</td>\n",
              "      <td>307.61</td>\n",
              "      <td>[{'productPrice': '44.99', 'productCategory': ...</td>\n",
              "      <td>C</td>\n",
              "      <td>5.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.145728e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b0d0ee4e-9472-467e-91bc-bdf77f8a84c0</td>\n",
              "      <td>a4260259-961a-4ab1-9dd6-50a9f0c05079</td>\n",
              "      <td>2019-08-04T03:32:40.740Z</td>\n",
              "      <td>2019-08-04T04:29:42.477Z</td>\n",
              "      <td>2019-08-04T04:00:05.579Z</td>\n",
              "      <td>240</td>\n",
              "      <td>CART_CHANGE</td>\n",
              "      <td>Cart</td>\n",
              "      <td>0</td>\n",
              "      <td>PBu</td>\n",
              "      <td>5</td>\n",
              "      <td>349.62</td>\n",
              "      <td>[{'productPrice': '39.99', 'productCategory': ...</td>\n",
              "      <td>P</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.049088e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8fa04b12-e187-405c-a30e-b27f677ed12f</td>\n",
              "      <td>4f723b51-a83d-4863-860d-f3f3419fcade</td>\n",
              "      <td>2019-08-04T03:55:34.356Z</td>\n",
              "      <td>2019-08-04T04:29:15.550Z</td>\n",
              "      <td>2019-08-04T04:00:06.161Z</td>\n",
              "      <td>420</td>\n",
              "      <td>PAGE_LOAD</td>\n",
              "      <td>Checkout</td>\n",
              "      <td>0</td>\n",
              "      <td>PBr</td>\n",
              "      <td>1</td>\n",
              "      <td>219.99</td>\n",
              "      <td>[{'productPrice': '219.99', 'productCategory':...</td>\n",
              "      <td>C</td>\n",
              "      <td>10.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.145728e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>304824f0-0556-4fda-a616-a27a1ad31dad</td>\n",
              "      <td>6b6e9f89-53c9-4005-9b60-a6e23c01c90f</td>\n",
              "      <td>2019-08-04T03:16:58.809Z</td>\n",
              "      <td>2019-08-04T04:29:36.673Z</td>\n",
              "      <td>2019-08-04T04:00:13.654Z</td>\n",
              "      <td>420</td>\n",
              "      <td>PAGE_LOAD</td>\n",
              "      <td>ProductSubCategory</td>\n",
              "      <td>0</td>\n",
              "      <td>PBr</td>\n",
              "      <td>1</td>\n",
              "      <td>39.99</td>\n",
              "      <td>[{'productPrice': '39.99', 'productCategory': ...</td>\n",
              "      <td>S</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.592000e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3c09349d-a803-4663-8dc3-2eb93732bb4e</td>\n",
              "      <td>92909fbd-b918-4b9b-aba8-a1d67493ba6f</td>\n",
              "      <td>2019-08-04T03:03:04.102Z</td>\n",
              "      <td>2019-08-04T04:29:04.603Z</td>\n",
              "      <td>2019-08-04T04:00:16.768Z</td>\n",
              "      <td>240</td>\n",
              "      <td>PAGE_LOAD</td>\n",
              "      <td>ProductSubCategory</td>\n",
              "      <td>0</td>\n",
              "      <td>PBu</td>\n",
              "      <td>2</td>\n",
              "      <td>79.32</td>\n",
              "      <td>[{'productPrice': '39.33', 'productCategory': ...</td>\n",
              "      <td>C</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.024634e+06</td>\n",
              "      <td>IGNORED</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                user_id                            session_id  \\\n",
              "0  785cc174-f549-4528-9ddf-6cbbd379212e  6777e217-dca4-4443-b60f-91ca4cf08136   \n",
              "1  b0d0ee4e-9472-467e-91bc-bdf77f8a84c0  a4260259-961a-4ab1-9dd6-50a9f0c05079   \n",
              "2  8fa04b12-e187-405c-a30e-b27f677ed12f  4f723b51-a83d-4863-860d-f3f3419fcade   \n",
              "3  304824f0-0556-4fda-a616-a27a1ad31dad  6b6e9f89-53c9-4005-9b60-a6e23c01c90f   \n",
              "4  3c09349d-a803-4663-8dc3-2eb93732bb4e  92909fbd-b918-4b9b-aba8-a1d67493ba6f   \n",
              "\n",
              "         session_start_time       session_expiry_time  \\\n",
              "0  2019-08-04T03:24:37.048Z  2019-08-04T04:29:38.875Z   \n",
              "1  2019-08-04T03:32:40.740Z  2019-08-04T04:29:42.477Z   \n",
              "2  2019-08-04T03:55:34.356Z  2019-08-04T04:29:15.550Z   \n",
              "3  2019-08-04T03:16:58.809Z  2019-08-04T04:29:36.673Z   \n",
              "4  2019-08-04T03:03:04.102Z  2019-08-04T04:29:04.603Z   \n",
              "\n",
              "                 event_time  event_time_zone   event_type           page_type  \\\n",
              "0  2019-08-04T03:59:59.963Z              180    PAGE_LOAD   ProductDetailPage   \n",
              "1  2019-08-04T04:00:05.579Z              240  CART_CHANGE                Cart   \n",
              "2  2019-08-04T04:00:06.161Z              420    PAGE_LOAD            Checkout   \n",
              "3  2019-08-04T04:00:13.654Z              420    PAGE_LOAD  ProductSubCategory   \n",
              "4  2019-08-04T04:00:16.768Z              240    PAGE_LOAD  ProductSubCategory   \n",
              "\n",
              "   offer_decline_count user_status  cart_quantity  cart_total  \\\n",
              "0                    0         PBu              7      307.61   \n",
              "1                    0         PBu              5      349.62   \n",
              "2                    0         PBr              1      219.99   \n",
              "3                    0         PBr              1       39.99   \n",
              "4                    0         PBu              2       79.32   \n",
              "\n",
              "                                           cart_data last_offer_type  \\\n",
              "0  [{'productPrice': '44.99', 'productCategory': ...               C   \n",
              "1  [{'productPrice': '39.99', 'productCategory': ...               P   \n",
              "2  [{'productPrice': '219.99', 'productCategory':...               C   \n",
              "3  [{'productPrice': '39.99', 'productCategory': ...               S   \n",
              "4  [{'productPrice': '39.33', 'productCategory': ...               C   \n",
              "\n",
              "   last_reward_value  last_spend_value  offer_display_count  user_screen_size  \\\n",
              "0                5.0             125.0                  1.0      3.145728e+06   \n",
              "1                5.0              50.0                  1.0      1.049088e+06   \n",
              "2               10.0             100.0                  1.0      3.145728e+06   \n",
              "3                5.0              50.0                  1.0      2.592000e+06   \n",
              "4                5.0              50.0                  1.0      1.024634e+06   \n",
              "\n",
              "  offer_acceptance_state  \n",
              "0                IGNORED  \n",
              "1                IGNORED  \n",
              "2                IGNORED  \n",
              "3                IGNORED  \n",
              "4                IGNORED  "
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rejoining the ID column with the Kaggle Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge the two DataFrames on index\n",
        "data_kaggle = pd.concat([first_column_df, new_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(875707, 20)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_kaggle.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting Time Columns to DateTime Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert object columns to datetime objects\n",
        "data_kaggle['session_start_time'] = pd.to_datetime(data_kaggle['session_start_time'])\n",
        "data_kaggle['session_expiry_time'] = pd.to_datetime(data_kaggle['session_expiry_time'])\n",
        "data_kaggle['event_time'] = pd.to_datetime(data_kaggle['event_time'])\n",
        "data_kaggle['event_time_zone'] = data_kaggle['event_time_zone'].astype('str')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing Zero or Near Zero Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "#mean is approx to 0 in 'offer_decline_count\"\n",
        "data_kaggle = data_kaggle.drop('offer_decline_count', axis=1)\n",
        "data_kaggle = data_kaggle.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding the Duration of each Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             session_id               session_start_time  \\\n",
            "0  6777e217-dca4-4443-b60f-91ca4cf08136 2019-08-04 03:24:37.048000+00:00   \n",
            "1  a4260259-961a-4ab1-9dd6-50a9f0c05079 2019-08-04 03:32:40.740000+00:00   \n",
            "2  4f723b51-a83d-4863-860d-f3f3419fcade 2019-08-04 03:55:34.356000+00:00   \n",
            "3  6b6e9f89-53c9-4005-9b60-a6e23c01c90f 2019-08-04 03:16:58.809000+00:00   \n",
            "4  92909fbd-b918-4b9b-aba8-a1d67493ba6f 2019-08-04 03:03:04.102000+00:00   \n",
            "\n",
            "               session_expiry_time  duration_minutes  \n",
            "0 2019-08-04 04:29:38.875000+00:00             82.08  \n",
            "1 2019-08-04 04:29:42.477000+00:00             86.12  \n",
            "2 2019-08-04 04:29:15.550000+00:00             38.64  \n",
            "3 2019-08-04 04:29:36.673000+00:00             90.64  \n",
            "4 2019-08-04 04:29:04.603000+00:00            119.72  \n"
          ]
        }
      ],
      "source": [
        "# Group by 'session_id' and calculate the duration\n",
        "session_durations = data_kaggle.groupby('session_id').agg({\n",
        "    'session_start_time': 'min',\n",
        "    'session_expiry_time': 'max'\n",
        "}).reset_index()\n",
        "\n",
        "# Calculate the duration as the difference between the max expiry time and the min start time\n",
        "session_durations['duration'] = session_durations['session_expiry_time'] - session_durations['session_start_time']\n",
        "\n",
        "# Convert duration to a more convenient unit: minutes\n",
        "session_durations['duration_minutes'] = session_durations['duration'].dt.total_seconds() / 60  # Convert seconds to minutes\n",
        "\n",
        "# Round the duration_minutes column to 2 decimal places (nearest 0.01)\n",
        "session_durations['duration_minutes'] = session_durations['duration_minutes'].round(2)\n",
        "\n",
        "# Now merge this back into the original dataframe\n",
        "data_kaggle = data_kaggle.merge(session_durations[['session_id', 'duration_minutes']], on='session_id', how='left')\n",
        "\n",
        "# Verify the results\n",
        "print(data_kaggle[['session_id', 'session_start_time', 'session_expiry_time', 'duration_minutes']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting the year, month, day, hour, and day name from session_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   year  month  day  hour day_of_week\n",
            "0  2019      8    4     3      Sunday\n",
            "1  2019      8    4     4      Sunday\n",
            "2  2019      8    4     4      Sunday\n",
            "3  2019      8    4     4      Sunday\n",
            "4  2019      8    4     4      Sunday\n"
          ]
        }
      ],
      "source": [
        "# Extract year, month, day, hour, and day name from session_start_time before normalizing\n",
        "data_kaggle['year'] = data_kaggle['event_time'].dt.year\n",
        "data_kaggle['month'] = data_kaggle['event_time'].dt.month\n",
        "data_kaggle['day'] = data_kaggle['event_time'].dt.day\n",
        "data_kaggle['hour'] = data_kaggle['event_time'].dt.hour  # Extract hour before normalizing\n",
        "data_kaggle['day_of_week'] = data_kaggle['event_time'].dt.day_name()\n",
        "\n",
        "# Now you have separate columns for year, month, day, hour, and day_of_week along with a combined string if needed\n",
        "print(data_kaggle[['year', 'month', 'day', 'hour', 'day_of_week']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding a Day Range based on the New 'day' column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   year  month  day  day_range  hour day_of_week\n",
            "0  2019      8    4          1     3      Sunday\n",
            "1  2019      8    4          1     4      Sunday\n",
            "2  2019      8    4          1     4      Sunday\n",
            "3  2019      8    4          1     4      Sunday\n",
            "4  2019      8    4          1     4      Sunday\n"
          ]
        }
      ],
      "source": [
        "# Function to categorize days with updated labels\n",
        "def categorize_day(day):\n",
        "    if 1 <= day <= 10:\n",
        "        return 1  # Updated label for days 1-10\n",
        "    elif 11 <= day <= 21:\n",
        "        return 2  # Updated label for days 11-21\n",
        "    else:\n",
        "        return 3  # Updated label for days 22-31\n",
        "\n",
        "# Apply the updated function to the 'day' column to create a new 'day_range' column with numerical labels\n",
        "data_kaggle['day_range'] = data_kaggle['day'].apply(categorize_day)\n",
        "\n",
        "# Display the first few rows to verify the new column\n",
        "print(data_kaggle[['year', 'month', 'day', 'day_range', 'hour', 'day_of_week']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting the new date columns to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming train is your DataFrame with 'event_time' column already converted to datetime\n",
        "data_kaggle['year'] = data_kaggle['event_time'].dt.year.astype(str)\n",
        "data_kaggle['month'] = data_kaggle['event_time'].dt.month.astype(str)\n",
        "data_kaggle['day'] = data_kaggle['event_time'].dt.day.astype(str)\n",
        "data_kaggle['hour'] = data_kaggle['event_time'].dt.hour.astype(str)\n",
        "data_kaggle['day_range'] = data_kaggle['day_range'].astype(str)\n",
        "\n",
        "#dropping irrelevant columns\n",
        "data_kaggle.drop(columns='day', axis=1, inplace=True)\n",
        "data_kaggle = data_kaggle.drop(columns=['session_start_time', 'session_expiry_time', 'event_time'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing the  User Screen Size Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_kaggle.drop(columns='user_screen_size', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making the 'cart_total' value for the same session consistent with the checkout one for that session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'train' is your DataFrame\n",
        "\n",
        "# Filter only the rows where 'page_type' is 'Checkout'\n",
        "checkout_rows = data_kaggle[data_kaggle['page_type'] == 'Checkout'][['session_id', 'cart_total']]\n",
        "\n",
        "# Group by 'session_id' and aggregate the 'cart_total' column\n",
        "checkout_rows = checkout_rows.groupby('session_id').first().reset_index()\n",
        "\n",
        "# Merge the filtered rows back to the original DataFrame\n",
        "data_kaggle = data_kaggle.merge(checkout_rows, on='session_id', suffixes=('', '_checkout'), how='left')\n",
        "\n",
        "# Replace 'cart_total' with the value from 'cart_total_checkout' where available\n",
        "data_kaggle['cart_total'] = data_kaggle['cart_total_checkout'].fillna(data_kaggle['cart_total'])\n",
        "\n",
        "# Drop the auxiliary column 'cart_total_checkout'\n",
        "data_kaggle.drop(columns=['cart_total_checkout'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing more Unecessecary Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_kaggle.drop(columns=['user_id','session_id', 'cart_data'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determening the Numerical and the Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['cart_quantity', 'cart_total', 'last_reward_value', 'last_spend_value',\n",
            "       'offer_display_count', 'total', 'duration_minutes'],\n",
            "      dtype='object')\n",
            "Index(['event_time_zone', 'event_type', 'page_type', 'user_status',\n",
            "       'last_offer_type', 'offer_acceptance_state', 'year', 'month', 'hour',\n",
            "       'day_of_week', 'day_range'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "numerical_cols_kaggle = train.select_dtypes(include=['int64', 'float64']).columns\n",
        "# Exclude the column named \"total\"\n",
        "#numerical_cols_kaggle = numerical_cols_kaggle[numerical_cols != 'total']\n",
        "categorical_cols_kaggle = train.select_dtypes(include=['object', 'category']).columns\n",
        "print(numerical_cols_kaggle)\n",
        "print(categorical_cols_kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing the Numerical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the numerical columns from the valid DataFrame\n",
        "numerical_df_valid = data_kaggle[numerical_cols_valid].copy()\n",
        "\n",
        "# Convert integer variables to floats if necessary\n",
        "numerical_df_valid = numerical_df_valid.astype(float)\n",
        "\n",
        "# Transform the numerical columns using the fitted scaler\n",
        "numerical_df_normalized_valid = pd.DataFrame(scaler.transform(numerical_df_valid), columns=numerical_cols_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One Hot Encoding the Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create a new DataFrame with selected categorical variables\n",
        "categorical_df_valid = data_kaggle[categorical_cols].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'categorical_df' is your DataFrame with only categorical features\n",
        "\n",
        "# Step 1: Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Step 2: Fit and transform the data\n",
        "categorical_df_encoded_valid = encoder.fit_transform(categorical_df_valid)\n",
        "\n",
        "# Step 3: Convert the encoded data to a DataFrame\n",
        "categorical_df_encoded_valid = pd.DataFrame(categorical_df_encoded_valid.toarray(), columns=encoder.get_feature_names_out(categorical_df_valid.columns))\n",
        "\n",
        "# 'categorical_df_encoded' is your new DataFrame with one-hot encoded categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joining the Preprocessed Numerical and Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_kaggle = pd.concat([numerical_df_normalized_valid, categorical_df_encoded_valid], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing the ID column from the Dataset to Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a new DataFrame with only the first column\n",
        "first_column_df = data_kaggle.iloc[:, :1]\n",
        "\n",
        "# Create a new DataFrame with all columns except the first column\n",
        "data_kaggle_test =data_kaggle.iloc[:, 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fixing Potential Columns Discrepencie between X_train and X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns present in X_train but not in X_test: {'event_time_zone_60', 'month_12', 'hour_20', 'month_11', 'month_10', 'month_9', 'event_time_zone_540', 'cart_quantity', 'month_2', 'event_time_zone_-630', 'month_1'}\n",
            "Columns present in X_test but not in X_train: {'hour_0', 'event_time_zone_-570', 'month_4', 'hour_23', 'offer_acceptance_state_nan', 'month_8', 'event_time_zone_-420', 'hour_22', 'event_time_zone_-780', 'event_time_zone_-240', 'event_time_zone_-600', 'hour_21', 'month_7', 'last_offer_type_nan'}\n"
          ]
        }
      ],
      "source": [
        "# Get the columns of X_train and X_test\n",
        "train_columns = set(X_train.columns)\n",
        "test_columns = set(data_kaggle_test.columns)\n",
        "\n",
        "# Columns present in X_train but not in X_test\n",
        "missing_in_test = train_columns - test_columns\n",
        "\n",
        "# Columns present in X_test but not in X_train\n",
        "missing_in_train = test_columns - train_columns\n",
        "\n",
        "print(\"Columns present in X_train but not in X_test:\", missing_in_test)\n",
        "print(\"Columns present in X_test but not in X_train:\", missing_in_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming X_train is your DataFrame and missing_in_test is your list of column names to drop\n",
        "X_train.drop(columns=missing_in_test, inplace=True)\n",
        "data_kaggle_test.drop(columns=missing_in_train, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Kaggle model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Model (Best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7491/7491\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 930us/step\n",
            "\u001b[1m27366/27366\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 944us/step\n",
            "Training RMSE: 443.85792919125004\n",
            "   test_predictions\n",
            "0        302.731964\n",
            "1         87.540878\n",
            "2        163.611389\n",
            "3         83.827530\n",
            "4        112.715942\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network architecture\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(35, activation='relu'),\n",
        "    Dense(1)  # Output layer with one neuron for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=116, batch_size=850, verbose=0)\n",
        "\n",
        "# Predict on training and testing data\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "test_predictions = model.predict(data_kaggle_test).flatten()\n",
        "\n",
        "# Compute RMSE\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
        "\n",
        "print(\"Training RMSE:\", train_rmse)\n",
        "\n",
        "# Create a DataFrame with the test predictions\n",
        "test_predictions_df = pd.DataFrame({'test_predictions': test_predictions})\n",
        "\n",
        "print(test_predictions_df.head())  # Display the first few rows of the DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Converting the Prediciton Dataset to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>302.731964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>87.540878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>163.611389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>83.827530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>112.715942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   test_predictions\n",
              "0        302.731964\n",
              "1         87.540878\n",
              "2        163.611389\n",
              "3         83.827530\n",
              "4        112.715942"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_predictions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_kaggle_ID = pd.read_csv(\"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\01_Education\\\\02_AUB\\\\02_Spring2024\\\\MSBA315-Introduction to Python for Machine Learning\\\\Assignments\\\\Assignment5\\\\AssignmentFolder\\\\data\\\\test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep only the first column\n",
        "data_kaggle_ID = data_kaggle_ID.iloc[:, [0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test= pd.read_csv('test.csv')\n",
        "\n",
        "# Create a DataFrame with the IDs and predictions\n",
        "result_df = pd.DataFrame({\n",
        "    'ID': data_kaggle_ID['ID'],  # Assuming 'id' is a column in X_test\n",
        "    'predicted_amount': test_predictions_df['test_predictions']\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "#result_df.to_csv('predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>predicted_amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>302.731964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>87.540878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>163.611389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>83.827530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>112.715942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  predicted_amount\n",
              "0   0        302.731964\n",
              "1   1         87.540878\n",
              "2   2        163.611389\n",
              "3   3         83.827530\n",
              "4   4        112.715942"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save the DataFrame to a CSV file\n",
        "result_df.to_csv('C:\\\\Users\\\\Lenovo\\\\Desktop\\\\predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "#result_df.to_csv('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\01_Education\\\\02_AUB\\\\02_Spring2024\\\\MSBA315-Introduction to Python for Machine Learning\\\\Assignments\\\\Assignment5\\\\AssignmentFolder\\\\data\\\\predictions.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
